#!/usr/bin/python
#coding:utf-8
#Author:Jean

from bs4 import BeautifulSoup
import requests
import re
import time 
from optparse import OptionParser 
from fake_useragent import UserAgent
from  tld import get_fld
import socket
from lib.GetAssetInfo import GetAsset


class Domain(object):
    def __init__(self,domain,port,page,filename):
        intclass=GetAsset(domain,port,page,filename)
        if not intclass.JudgeIP(domain):
            domain="http://"+domain
            self.domain=get_fld(domain)
        else:
            self.domain=domain
        self.port=port
        self.page=page
        self.filename=filename

    def GetHeaders(self):
        ua = UserAgent()
        header ={
            "Connection":"keep-alive",
            "Upgrade-Insecure-Requests":"1",
            "User-Agent":"%s"%ua.random,
            "Accept":"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8",
            "Accept-Encoding":"gzip,deflate",
            "Accept-Language":"zh-CN,zh;q=0.9"
            }
        return header

    def GetDomainFromsogou(self):
        print("#######################################################Sogou##########################\n")
        result=[]
        file=open(self.filename,"w+")
        ua = UserAgent()
        try:
            for j in range(1,int(self.page)+1):
                p=(j-1)*10
                url="https://www.sogou.com/web?query=site%%3A%s&from=index-no\
                login&s_from=index&sut=6694&sst0=1609987344627&lkt=15%%2C1609987337934%%2C1609987344355&\
                sugsuv=1609987334545270&sugtime=1609987344627&page=%s&ie=utf8&p=40040100&dp=1&w=01019900&dr=1" %(self.domain,p)
                print("获取site:%s第%s页的内容.................."%(self.domain,j))
                #print(url)
                data=requests.get(url=url,headers=self.GetHeaders())
                soup=BeautifulSoup(data.text.replace(u'\xa0', u' '),'lxml')
                tag=soup.find_all('div',attrs={'class':'fb'})
                for i in tag:
                    pattern=re.compile(r"(\w+\.){1,}%s"%self.domain)
                    cont=i.cite.text.replace(u'\xa0', u' ').replace(" ","")
                    sogoudomain=pattern.search(cont)
                    print(sogoudomain.group())
                    result.append(str(sogoudomain.group()))
                    file.write(str(sogoudomain.group())+"\n")
                time.sleep(5)
        except Exception as e:
            print(e)
        return result
        file.close()


    def GetDomainFromBaidu(self):
        print("#######################################################Baidu##########################\n")
        result=[]
        file=open(self.filename,"w+")
        try:
            for j in range(1,int(self.page)+1):
                p=(j-1)*10
                url="https://www.baidu.com/s?wd=site%%3A%s&pn=%s&oq=site%%3A%s\
                &ie=utf-8&fenlei=256&rsv_idx=1&rsv_pq=ee5b2a33000815e7&\
                rsv_t=685afsrI5MwzKqY7Q54co3Z4umvdAM0BApUIv7IX%%2B761hYc1O8%%2FyMog2fXk&rsv_page=1" %(self.domain,p,self.domain)
                print("获取site:%s第%s页的内容.................................."%(self.domain,j))  
                data=requests.get(url=url,headers=self.GetHeaders())
                soup=BeautifulSoup(data.text,'lxml')
                tag=soup.find_all('a',attrs={'class':'c-showurl c-color-gray'})
                for i in tag:
                    getdomain=str(i.string).split('/')[0]
                    if self.domain in  getdomain:
                        if getdomain not in result:
                            print(getdomain)
                            result.append(getdomain)
                            file.write(getdomain+'\n')
                    else:
                        continue
                time.sleep(3)
        except Exception as e:
            print(e)
        #print(result)
        file.close()
        return result

    def GetDomainFromBing(self):
        print("#######################################################Bing##########################\n")
        resultbing=[]
        file=open(self.filename,"w+")
        ua = UserAgent()
        try:
            for j in range(1,int(self.page)+1):
                p=(j-1)*10
                print("获取site:%s第%s页的内容.................................."%(self.domain,j))
                url="https://cn.bing.com/search?q=site%%3a%s&qs=n&\
                sp=-1&pq=site%%3a%s&sc=1-14&sk=&cvid=F13445D401B348\
                2FBAF018C1403CDFC4&first=%s&FORM=PORE" %(self.domain,self.domain,p)
                print(url)
                data=requests.get(url=url,headers=self.GetHeaders())
                soup=BeautifulSoup(data.text,'lxml')
                tag=soup.find_all('h2')
                for i in tag:
                    #print(i.a['href'])
                    resulturl=i.a['href']
                    patt=re.compile(r"(\w+\.){1,}%s"%self.domain)
                    conten=patt.search(resulturl)
                    #print(conten.group())  #str
                    resultbing.append(conten.group())
                    #file.write(conten.group()+"\n")
                time.sleep(3)
        except Exception as e:
            print(e)
        file.close()
        return resultbing

    def GetDomainFrom360(self):
        print("#######################################################360##########################\n")
        result=[]
        file=open(self.filename,"w+")
        ua = UserAgent()
        try:
            for j in range(1,int(self.page)+1):
                p=(j-1)*10
                url="https://www.so.com/s?q=site%%3A%s&pn=%s&psid=001266ee55b3cd61dbc2c072b5e8820a&src=srp_paging&fr=none" %(self.domain,p)
                print("获取site:%s第%s页的内容.................."%(self.domain,j))
                data=requests.get(url=url,headers=self.GetHeaders())
                soup=BeautifulSoup(data.text,'lxml')
                tag=soup.find_all('p',attrs={'class':'g-linkinfo'})
                for i in tag:
                    #print(i.cite.text)
                    sourl=i.cite.text
                    sopattern=re.compile(r"(\w+\.){1,}%s"%self.domain)
                    sodomain=sopattern.search(sourl)
                    result.append(str(sodomain.group()))
                    print(str(sodomain.group()))
                    file.write(str(sodomain.group())+"\n")
                time.sleep(3)
        except Exception as e:
            print(e)
        #print(result)
        file.close()
        return result
    def GetDomainFinal(self):
        #finalresult=''
        file=open(self.filename,"w+")
        domainresult=[]
        baiduresult=[]
        bingresult=[]
        result360=[]
        sogouresult=[]
        try:
            baiduresult=self.GetDomainFromBaidu()
            bingresult=self.GetDomainFromBing()
            result360=self.GetDomainFrom360()
            sogouresult=self.GetDomainFromsogou()
            domainresult.extend(baiduresult)
            domainresult.extend(bingresult)
            domainresult.extend(result360)
            domainresult.extend(sogouresult)
        except Exception as e:
            print(e)
        domainresult=list(set(domainresult))
        #domainresult=list(set(domainresult)).sort()
        print(domainresult)
        #if domainresult:
            #for k in domainresult:
                #file.write(k+"\n")
                #finalresult+=(k+"<br>")
        #else:
            #domainresult=['未获取到子域名']
            #finalresult="未获取到子域名"
        #file.close()
        return domainresult

    def GetSubDomainInfo(self):
        iplist=[]
        finalresult=''
        try:
            finurl=self.GetDomainFinal()
            print(finurl)
            for w in finurl:
                resultip1 = socket.getaddrinfo(w,'http')
                print(w+"------"+resultip1[0][4][0])
                finalresult+=(w+"------"+resultip1[0][4][0]+"<br>")
                iplist.append(resultip1[0][4][0])
                    #iplist.append(resultip[0][4][0])
            finalresult+="<br>"
            for domainip in iplist:
                finalresult+=(domainip+"<br>")
        except Exception as e:
            print(e)
        return finalresult

    def GetDomainHistoryIP(self):
        getasset=GetAsset(self.domain,self.port,self.page,self.filename)
        iplist=[]
        finalresult=''
        try:
            finurl=self.GetDomainFinal()
            print(finurl)
            for w in finurl:
                resultip1 = socket.getaddrinfo(w,'http')
                #print(w+"------"+resultip1[0][4][0])
                #finalresult+=(w+"------"+resultip1[0][4][0]+"<br>")
                iplist.append(resultip1[0][4][0])
                    #iplist.append(resultip[0][4][0])
            #finalresult+="<br>"
                finalresult+=(w+"---历史解析/绑定"+"<br>"+getasset.GetIPinfo()+"<br>")

        except Exception as e:
            print(e)
        return finalresult